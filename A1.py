import os,io,requests,re
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont

#â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”æ·»åŠ æ°´å°â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def add_watermark(img_bytes: bytes, text: str, pos: int = 4) -> bytes:

    img = Image.open(io.BytesIO(img_bytes)).convert("RGBA")
    draw = ImageDraw.Draw(img)

    fsize = max(12, int(img.width * 0.035))
    try:
        font_path = os.path.join(os.path.dirname(__file__), "data/SourceHanSansHWSC-Regular.otf")   #ç”¨dataé‡Œçš„æ€æºé»‘ä½“
        font = ImageFont.truetype(font_path, fsize)
    except OSError:
        font = ImageFont.load_default()

    # Pillow â‰¥10 ç”¨ textbbox è®¡ç®—æ–‡å­—å°ºå¯¸
    # è¿”å› (x0, y0, x1, y1)
    bbox = draw.textbbox((0, 0), text, font=font)
    w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]

    pos_tbl = {
        1: (10, 10),
        2: (10, img.height - h - 10),
        3: (img.width - w - 10, 10),
        4: (img.width - w - 10, img.height - h - 10)
    }
    draw.text(pos_tbl[pos], text, fill=(255, 255, 255, 160), font=font)
    img = img.convert("RGB")
    out = io.BytesIO()
    img.save(out, format="JPEG", quality=95)
    return out.getvalue()



#â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ä¸‹è½½ç¨‹åºâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def download(keyword,pn,startid,pos):
    #â€”â€”â€”â€”â€”â€”â€”â€”å‚æ•°â€”â€”â€”â€”â€”â€”â€”â€”
    url =f"https://image.baidu.com/search/index?word={keyword}&pn={pn}"     #ç™¾åº¦ä¸‹è½½åœ°å€
    ua ={
        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0',
        'Referer':'https://image.baidu.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'
        }                                                                            #å®šä¹‰ua
    resp = requests.get(url,headers=ua)                                              #å‘é€è¯·æ±‚
    water_txt = "ç½‘å®‰2301 åˆ˜æ¯…å‡¡ 34"                                                   #æ°´å°å‚æ•°
    save_dir = os.path.join(os.path.expanduser("~/Downloads/len5010"), keyword)      #é»˜è®¤ä¸‹è½½è·¯å¾„
    os.makedirs(save_dir, exist_ok=True)                                             #æ²¡æœ‰å°±è‡ªåŠ¨åˆ›å»º

    #â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”æŠ“urlâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    html_txt = resp.text          #è·å–çš„å†…å®¹
    img_urls = []                 #åŒ¹é…çš„ç›®æ ‡å›¾ç‰‡url
    print(f"ç¬¬{pn+1}é¡µè®¿é—®çŠ¶æ€:{resp.status_code}")        #æŸ¥çœ‹è¯·æ±‚çš„çŠ¶æ€ 
    html_soup = BeautifulSoup(html_txt, 'html.parser')
    html_sc = html_soup.find_all('script',string=re.compile(r'"objurl":'))  #ä½¿ç”¨bsæŸ¥æ‰¾åŒ…å«objurlçš„å­—æ®µ
    for html_txt in html_sc:
        html_target = html_txt.string      #æŒ‡å‘çš„target
        if not html_target:
            continue                       #æ²¡æœ‰url
        url_target = r'"objurl":"(https?://[^"&]+?\.jpg)"'  #é™å®š.jpg ç»“å°¾ï¼Œæ’é™¤éæ³•å­—ç¬¦
        matches = re.findall(url_target, html_target)
        if matches:
            img_urls.extend(matches)    #åœ¨æ€»çš„htmlæ–‡æœ¬é‡Œé¢åŒ¹é…éœ€è¦çš„å­—æ®µ
        if not img_urls:
            print("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
        else:
            print(f"æˆåŠŸåŒ¹é… {len(img_urls)} æ¡ .jpg é“¾æ¥ï¼š")   

    #-â€”â€”â€”â€”ä¸‹è½½å¹¶åŠ æ°´å°
    good, bad = 0, 0
    for i, link in enumerate(img_urls, 1):
        try:
            r = requests.get(link, headers=ua, timeout=10)
            r.raise_for_status()
            # åŠ æ°´å°
            wm_img = add_watermark(r.content, water_txt, pos)
            # æ–‡ä»¶å
            idx   = startid + i            
            fname = os.path.join(save_dir, f"åˆ˜æ¯…å‡¡{idx}.jpg")
            with open(fname, 'wb') as f:
                f.write(wm_img)
            good += 1
            print(f"[{good}/{len(img_urls)}] å·²ä¿å­˜ {fname}")
        except Exception as e:
            bad += 1
            print(f"Ã— ä¸‹è½½å¤±è´¥({bad})ï¼š{link} -> {e}")

    print(f"å®Œæˆï¼æˆåŠŸ {good} å¼ ï¼Œå¤±è´¥ {bad} å¼ ï¼Œæ–‡ä»¶åœ¨ {save_dir}/")
    return good


#â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ä¸»è¿›ç¨‹â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
keyword = input("è¯·è¾“å…¥å…³é”®è¯(è¯·è¾“å…¥éè¶ğŸ’¢)ï¼š").strip()
pages   = int(input("è¯·è¾“å…¥è¦ä¸‹è½½çš„é¡µæ•°ï¼š"))       
pos  = int(input("è¯·è¾“å…¥æ°´å°ä½ç½®ï¼š"))        

seq = 0                                    
for page in range(pages):                                 
    got = download(keyword, page, seq,pos)        
    seq += got                    

print("å…¨éƒ¨é¡µé¢å¤„ç†å®Œæ¯•")